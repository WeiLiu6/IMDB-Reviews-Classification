{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "path = \"C://Users//weili//Projects//Bagofwords//Data//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+\"labeledTrainData.tsv\", header=0, delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv(path+\"testData.tsv\", header=0, delimiter='\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv(path+\"unlabeledTrainData.tsv\", header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(raw_review, remove_stopwords=False):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, 'lxml').get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters and non-numbers, then turn into lower case and split them\n",
    "    words = re.sub(\"[^0-9a-zA-Z]\", \" \", review_text).lower().split()\n",
    "    # 3. If choose to remove stopwords, remove them\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stops]\n",
    "   \n",
    "    # Return a list of words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    # 2. Loop over each sentence\n",
    "    sentence_list = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentence_list.append(review_to_wordlist(raw_sentence, remove_stopwords=False))\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentence_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabel training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "print('Parsing sentences from training set')\n",
    "for review in train['review']:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "print('Parsing sentences from unlabel training set')\n",
    "for review in unlabeled_train['review']:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "len(sentences)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-29 23:56:43,918 : INFO : collecting all words and their counts\n",
      "2018-09-29 23:56:43,920 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-09-29 23:56:43,951 : INFO : PROGRESS: at sentence #10000, processed 227240 words, keeping 18038 word types\n",
      "2018-09-29 23:56:43,980 : INFO : PROGRESS: at sentence #20000, processed 454577 words, keeping 25324 word types\n",
      "2018-09-29 23:56:44,008 : INFO : PROGRESS: at sentence #30000, processed 675275 words, keeping 30478 word types\n",
      "2018-09-29 23:56:44,039 : INFO : PROGRESS: at sentence #40000, processed 903015 words, keeping 34863 word types\n",
      "2018-09-29 23:56:44,068 : INFO : PROGRESS: at sentence #50000, processed 1123504 words, keeping 38329 word types\n",
      "2018-09-29 23:56:44,098 : INFO : PROGRESS: at sentence #60000, processed 1346265 words, keeping 41338 word types\n",
      "2018-09-29 23:56:44,129 : INFO : PROGRESS: at sentence #70000, processed 1570739 words, keeping 43986 word types\n",
      "2018-09-29 23:56:44,160 : INFO : PROGRESS: at sentence #80000, processed 1791249 words, keeping 46400 word types\n",
      "2018-09-29 23:56:44,191 : INFO : PROGRESS: at sentence #90000, processed 2016723 words, keeping 48869 word types\n",
      "2018-09-29 23:56:44,219 : INFO : PROGRESS: at sentence #100000, processed 2239896 words, keeping 50980 word types\n",
      "2018-09-29 23:56:44,250 : INFO : PROGRESS: at sentence #110000, processed 2460901 words, keeping 52890 word types\n",
      "2018-09-29 23:56:44,278 : INFO : PROGRESS: at sentence #120000, processed 2684304 words, keeping 54967 word types\n",
      "2018-09-29 23:56:44,310 : INFO : PROGRESS: at sentence #130000, processed 2911246 words, keeping 56741 word types\n",
      "2018-09-29 23:56:44,337 : INFO : PROGRESS: at sentence #140000, processed 3125257 words, keeping 58290 word types\n",
      "2018-09-29 23:56:44,367 : INFO : PROGRESS: at sentence #150000, processed 3352187 words, keeping 60038 word types\n",
      "2018-09-29 23:56:44,398 : INFO : PROGRESS: at sentence #160000, processed 3576077 words, keeping 61632 word types\n",
      "2018-09-29 23:56:44,428 : INFO : PROGRESS: at sentence #170000, processed 3800671 words, keeping 63128 word types\n",
      "2018-09-29 23:56:44,460 : INFO : PROGRESS: at sentence #180000, processed 4022558 words, keeping 64575 word types\n",
      "2018-09-29 23:56:44,489 : INFO : PROGRESS: at sentence #190000, processed 4249063 words, keeping 65900 word types\n",
      "2018-09-29 23:56:44,520 : INFO : PROGRESS: at sentence #200000, processed 4474464 words, keeping 67217 word types\n",
      "2018-09-29 23:56:44,550 : INFO : PROGRESS: at sentence #210000, processed 4697223 words, keeping 68546 word types\n",
      "2018-09-29 23:56:44,581 : INFO : PROGRESS: at sentence #220000, processed 4923567 words, keeping 69884 word types\n",
      "2018-09-29 23:56:44,612 : INFO : PROGRESS: at sentence #230000, processed 5147437 words, keeping 71164 word types\n",
      "2018-09-29 23:56:44,644 : INFO : PROGRESS: at sentence #240000, processed 5376270 words, keeping 72402 word types\n",
      "2018-09-29 23:56:44,672 : INFO : PROGRESS: at sentence #250000, processed 5591638 words, keeping 73620 word types\n",
      "2018-09-29 23:56:44,703 : INFO : PROGRESS: at sentence #260000, processed 5812901 words, keeping 74778 word types\n",
      "2018-09-29 23:56:44,732 : INFO : PROGRESS: at sentence #270000, processed 6035538 words, keeping 76111 word types\n",
      "2018-09-29 23:56:44,764 : INFO : PROGRESS: at sentence #280000, processed 6262672 words, keeping 77737 word types\n",
      "2018-09-29 23:56:44,794 : INFO : PROGRESS: at sentence #290000, processed 6487099 words, keeping 79229 word types\n",
      "2018-09-29 23:56:44,825 : INFO : PROGRESS: at sentence #300000, processed 6713049 words, keeping 80594 word types\n",
      "2018-09-29 23:56:44,856 : INFO : PROGRESS: at sentence #310000, processed 6939616 words, keeping 81947 word types\n",
      "2018-09-29 23:56:44,887 : INFO : PROGRESS: at sentence #320000, processed 7165783 words, keeping 83307 word types\n",
      "2018-09-29 23:56:44,919 : INFO : PROGRESS: at sentence #330000, processed 7388849 words, keeping 84568 word types\n",
      "2018-09-29 23:56:44,950 : INFO : PROGRESS: at sentence #340000, processed 7619625 words, keeping 85844 word types\n",
      "2018-09-29 23:56:44,981 : INFO : PROGRESS: at sentence #350000, processed 7844224 words, keeping 87024 word types\n",
      "2018-09-29 23:56:45,014 : INFO : PROGRESS: at sentence #360000, processed 8066036 words, keeping 88224 word types\n",
      "2018-09-29 23:56:45,043 : INFO : PROGRESS: at sentence #370000, processed 8294461 words, keeping 89358 word types\n",
      "2018-09-29 23:56:45,074 : INFO : PROGRESS: at sentence #380000, processed 8520928 words, keeping 90551 word types\n",
      "2018-09-29 23:56:45,106 : INFO : PROGRESS: at sentence #390000, processed 8752027 words, keeping 91609 word types\n",
      "2018-09-29 23:56:45,137 : INFO : PROGRESS: at sentence #400000, processed 8976260 words, keeping 92659 word types\n",
      "2018-09-29 23:56:45,169 : INFO : PROGRESS: at sentence #410000, processed 9198885 words, keeping 93646 word types\n",
      "2018-09-29 23:56:45,199 : INFO : PROGRESS: at sentence #420000, processed 9421275 words, keeping 94699 word types\n",
      "2018-09-29 23:56:45,231 : INFO : PROGRESS: at sentence #430000, processed 9650226 words, keeping 95740 word types\n",
      "2018-09-29 23:56:45,262 : INFO : PROGRESS: at sentence #440000, processed 9878340 words, keeping 96739 word types\n",
      "2018-09-29 23:56:45,293 : INFO : PROGRESS: at sentence #450000, processed 10103333 words, keeping 97893 word types\n",
      "2018-09-29 23:56:45,326 : INFO : PROGRESS: at sentence #460000, processed 10337463 words, keeping 98968 word types\n",
      "2018-09-29 23:56:45,357 : INFO : PROGRESS: at sentence #470000, processed 10566647 words, keeping 99845 word types\n",
      "2018-09-29 23:56:45,389 : INFO : PROGRESS: at sentence #480000, processed 10788293 words, keeping 100796 word types\n",
      "2018-09-29 23:56:45,419 : INFO : PROGRESS: at sentence #490000, processed 11016456 words, keeping 101878 word types\n",
      "2018-09-29 23:56:45,451 : INFO : PROGRESS: at sentence #500000, processed 11239432 words, keeping 102801 word types\n",
      "2018-09-29 23:56:45,481 : INFO : PROGRESS: at sentence #510000, processed 11466015 words, keeping 103750 word types\n",
      "2018-09-29 23:56:45,512 : INFO : PROGRESS: at sentence #520000, processed 11690736 words, keeping 104669 word types\n",
      "2018-09-29 23:56:45,543 : INFO : PROGRESS: at sentence #530000, processed 11916464 words, keeping 105502 word types\n",
      "2018-09-29 23:56:45,575 : INFO : PROGRESS: at sentence #540000, processed 12142447 words, keeping 106401 word types\n",
      "2018-09-29 23:56:45,608 : INFO : PROGRESS: at sentence #550000, processed 12369300 words, keeping 107290 word types\n",
      "2018-09-29 23:56:45,637 : INFO : PROGRESS: at sentence #560000, processed 12591871 words, keeping 108175 word types\n",
      "2018-09-29 23:56:45,672 : INFO : PROGRESS: at sentence #570000, processed 12822238 words, keeping 108982 word types\n",
      "2018-09-29 23:56:45,703 : INFO : PROGRESS: at sentence #580000, processed 13045003 words, keeping 109872 word types\n",
      "2018-09-29 23:56:45,733 : INFO : PROGRESS: at sentence #590000, processed 13271738 words, keeping 110743 word types\n",
      "2018-09-29 23:56:45,763 : INFO : PROGRESS: at sentence #600000, processed 13495259 words, keeping 111494 word types\n",
      "2018-09-29 23:56:45,794 : INFO : PROGRESS: at sentence #610000, processed 13717559 words, keeping 112396 word types\n",
      "2018-09-29 23:56:45,826 : INFO : PROGRESS: at sentence #620000, processed 13945321 words, keeping 113170 word types\n",
      "2018-09-29 23:56:45,857 : INFO : PROGRESS: at sentence #630000, processed 14170897 words, keeping 113969 word types\n",
      "2018-09-29 23:56:45,889 : INFO : PROGRESS: at sentence #640000, processed 14392921 words, keeping 114809 word types\n",
      "2018-09-29 23:56:45,920 : INFO : PROGRESS: at sentence #650000, processed 14620123 words, keeping 115618 word types\n",
      "2018-09-29 23:56:45,951 : INFO : PROGRESS: at sentence #660000, processed 14844191 words, keeping 116394 word types\n",
      "2018-09-29 23:56:45,981 : INFO : PROGRESS: at sentence #670000, processed 15068851 words, keeping 117115 word types\n",
      "2018-09-29 23:56:46,012 : INFO : PROGRESS: at sentence #680000, processed 15295014 words, keeping 117840 word types\n",
      "2018-09-29 23:56:46,042 : INFO : PROGRESS: at sentence #690000, processed 15518432 words, keeping 118637 word types\n",
      "2018-09-29 23:56:46,075 : INFO : PROGRESS: at sentence #700000, processed 15748360 words, keeping 119468 word types\n",
      "2018-09-29 23:56:46,105 : INFO : PROGRESS: at sentence #710000, processed 15972643 words, keeping 120130 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-29 23:56:46,137 : INFO : PROGRESS: at sentence #720000, processed 16199229 words, keeping 120765 word types\n",
      "2018-09-29 23:56:46,168 : INFO : PROGRESS: at sentence #730000, processed 16426860 words, keeping 121513 word types\n",
      "2018-09-29 23:56:46,199 : INFO : PROGRESS: at sentence #740000, processed 16649236 words, keeping 122242 word types\n",
      "2018-09-29 23:56:46,230 : INFO : PROGRESS: at sentence #750000, processed 16868896 words, keeping 122892 word types\n",
      "2018-09-29 23:56:46,260 : INFO : PROGRESS: at sentence #760000, processed 17089573 words, keeping 123538 word types\n",
      "2018-09-29 23:56:46,292 : INFO : PROGRESS: at sentence #770000, processed 17318060 words, keeping 124325 word types\n",
      "2018-09-29 23:56:46,326 : INFO : PROGRESS: at sentence #780000, processed 17549563 words, keeping 125051 word types\n",
      "2018-09-29 23:56:46,356 : INFO : PROGRESS: at sentence #790000, processed 17777883 words, keeping 125739 word types\n",
      "2018-09-29 23:56:46,374 : INFO : collected 126186 word types from a corpus of 17901685 raw words and 795538 sentences\n",
      "2018-09-29 23:56:46,375 : INFO : Loading a fresh vocabulary\n",
      "2018-09-29 23:56:46,784 : INFO : min_count=40 retains 16731 unique words (13% of original 126186, drops 109455)\n",
      "2018-09-29 23:56:46,785 : INFO : min_count=40 leaves 17335523 word corpus (96% of original 17901685, drops 566162)\n",
      "2018-09-29 23:56:46,812 : INFO : deleting the raw counts dictionary of 126186 items\n",
      "2018-09-29 23:56:46,815 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-09-29 23:56:46,816 : INFO : downsampling leaves estimated 12862720 word corpus (74.2% of prior 17335523)\n",
      "2018-09-29 23:56:46,850 : INFO : estimated required memory for 16731 words and 300 dimensions: 48519900 bytes\n",
      "2018-09-29 23:56:46,851 : INFO : resetting layer weights\n",
      "2018-09-29 23:56:47,007 : INFO : training model with 4 workers on 16731 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-09-29 23:56:48,015 : INFO : EPOCH 1 - PROGRESS: at 12.38% examples, 1579376 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:49,017 : INFO : EPOCH 1 - PROGRESS: at 24.85% examples, 1584088 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:50,019 : INFO : EPOCH 1 - PROGRESS: at 37.23% examples, 1583813 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:51,021 : INFO : EPOCH 1 - PROGRESS: at 49.55% examples, 1585238 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:52,023 : INFO : EPOCH 1 - PROGRESS: at 61.94% examples, 1588440 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:53,027 : INFO : EPOCH 1 - PROGRESS: at 74.35% examples, 1589446 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:54,028 : INFO : EPOCH 1 - PROGRESS: at 85.23% examples, 1561921 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:55,031 : INFO : EPOCH 1 - PROGRESS: at 97.50% examples, 1563234 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:55,228 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-09-29 23:56:55,233 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-29 23:56:55,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-29 23:56:55,243 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-29 23:56:55,243 : INFO : EPOCH - 1 : training on 17901685 raw words (12862431 effective words) took 8.2s, 1562678 effective words/s\n",
      "2018-09-29 23:56:56,247 : INFO : EPOCH 2 - PROGRESS: at 12.28% examples, 1570759 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:57,248 : INFO : EPOCH 2 - PROGRESS: at 24.53% examples, 1566334 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:58,249 : INFO : EPOCH 2 - PROGRESS: at 36.80% examples, 1567275 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:56:59,251 : INFO : EPOCH 2 - PROGRESS: at 48.93% examples, 1567218 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:00,253 : INFO : EPOCH 2 - PROGRESS: at 60.88% examples, 1562494 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:01,257 : INFO : EPOCH 2 - PROGRESS: at 73.13% examples, 1564347 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:02,258 : INFO : EPOCH 2 - PROGRESS: at 85.29% examples, 1563929 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:03,261 : INFO : EPOCH 2 - PROGRESS: at 97.39% examples, 1562340 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:03,467 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-09-29 23:57:03,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-29 23:57:03,469 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-29 23:57:03,472 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-29 23:57:03,473 : INFO : EPOCH - 2 : training on 17901685 raw words (12861499 effective words) took 8.2s, 1563644 effective words/s\n",
      "2018-09-29 23:57:04,481 : INFO : EPOCH 3 - PROGRESS: at 12.10% examples, 1542415 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:05,481 : INFO : EPOCH 3 - PROGRESS: at 24.41% examples, 1556719 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:06,484 : INFO : EPOCH 3 - PROGRESS: at 36.74% examples, 1562382 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:07,486 : INFO : EPOCH 3 - PROGRESS: at 48.61% examples, 1554260 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:08,487 : INFO : EPOCH 3 - PROGRESS: at 60.83% examples, 1560141 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:09,492 : INFO : EPOCH 3 - PROGRESS: at 72.69% examples, 1553813 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:10,496 : INFO : EPOCH 3 - PROGRESS: at 83.85% examples, 1535737 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:11,501 : INFO : EPOCH 3 - PROGRESS: at 96.18% examples, 1540874 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:11,808 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-09-29 23:57:11,814 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-29 23:57:11,817 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-29 23:57:11,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-29 23:57:11,823 : INFO : EPOCH - 3 : training on 17901685 raw words (12862300 effective words) took 8.3s, 1541087 effective words/s\n",
      "2018-09-29 23:57:12,831 : INFO : EPOCH 4 - PROGRESS: at 12.22% examples, 1557622 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:13,831 : INFO : EPOCH 4 - PROGRESS: at 24.53% examples, 1564723 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:14,838 : INFO : EPOCH 4 - PROGRESS: at 36.96% examples, 1569420 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:15,839 : INFO : EPOCH 4 - PROGRESS: at 49.09% examples, 1569384 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:16,844 : INFO : EPOCH 4 - PROGRESS: at 61.21% examples, 1567942 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:17,844 : INFO : EPOCH 4 - PROGRESS: at 73.24% examples, 1564956 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:18,849 : INFO : EPOCH 4 - PROGRESS: at 85.06% examples, 1557668 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:19,849 : INFO : EPOCH 4 - PROGRESS: at 96.95% examples, 1554003 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:20,098 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-09-29 23:57:20,104 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-29 23:57:20,105 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-29 23:57:20,111 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-29 23:57:20,111 : INFO : EPOCH - 4 : training on 17901685 raw words (12863488 effective words) took 8.3s, 1552778 effective words/s\n",
      "2018-09-29 23:57:21,118 : INFO : EPOCH 5 - PROGRESS: at 11.83% examples, 1508825 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:22,121 : INFO : EPOCH 5 - PROGRESS: at 24.24% examples, 1545921 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:23,121 : INFO : EPOCH 5 - PROGRESS: at 36.57% examples, 1556073 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:24,120 : INFO : EPOCH 5 - PROGRESS: at 48.66% examples, 1557804 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:25,125 : INFO : EPOCH 5 - PROGRESS: at 60.67% examples, 1556021 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:26,132 : INFO : EPOCH 5 - PROGRESS: at 71.30% examples, 1523323 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-29 23:57:27,135 : INFO : EPOCH 5 - PROGRESS: at 83.52% examples, 1529264 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:28,140 : INFO : EPOCH 5 - PROGRESS: at 95.85% examples, 1535413 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-29 23:57:28,472 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-09-29 23:57:28,476 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-29 23:57:28,479 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-29 23:57:28,480 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-29 23:57:28,481 : INFO : EPOCH - 5 : training on 17901685 raw words (12861148 effective words) took 8.4s, 1537479 effective words/s\n",
      "2018-09-29 23:57:28,481 : INFO : training on a 89508425 raw words (64310866 effective words) took 41.5s, 1550661 effective words/s\n",
      "2018-09-29 23:57:28,482 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-09-29 23:57:28,547 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-09-29 23:57:28,548 : INFO : not storing attribute vectors_norm\n",
      "2018-09-29 23:57:28,549 : INFO : not storing attribute cum_table\n",
      "2018-09-29 23:57:29,025 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "num_features = 300   # Word vector dimensionality                      \n",
    "min_wordcount = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_wordcount, window=context, sample=downsampling, seed=1)\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'and',\n",
       " 'a',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'it',\n",
       " 'in',\n",
       " 'i',\n",
       " 'this',\n",
       " 'that',\n",
       " 's',\n",
       " 'was',\n",
       " 'as',\n",
       " 'with',\n",
       " 'for',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'film',\n",
       " 'you',\n",
       " 't',\n",
       " 'on',\n",
       " 'not',\n",
       " 'he',\n",
       " 'are',\n",
       " 'his',\n",
       " 'have',\n",
       " 'be',\n",
       " 'one',\n",
       " 'all',\n",
       " 'at',\n",
       " 'they',\n",
       " 'by',\n",
       " 'who',\n",
       " 'an',\n",
       " 'from',\n",
       " 'so',\n",
       " 'like',\n",
       " 'there',\n",
       " 'her',\n",
       " 'or',\n",
       " 'just',\n",
       " 'about',\n",
       " 'out',\n",
       " 'has',\n",
       " 'if',\n",
       " 'what',\n",
       " 'some',\n",
       " 'good',\n",
       " 'can',\n",
       " 'more',\n",
       " 'when',\n",
       " 'very',\n",
       " 'she',\n",
       " 'up',\n",
       " 'no',\n",
       " 'time',\n",
       " 'even',\n",
       " 'would',\n",
       " 'my',\n",
       " 'which',\n",
       " 'their',\n",
       " 'story',\n",
       " 'only',\n",
       " 'really',\n",
       " 'see',\n",
       " 'had',\n",
       " 'were',\n",
       " 'well',\n",
       " 'we',\n",
       " 'me',\n",
       " 'than',\n",
       " 'much',\n",
       " 'bad',\n",
       " 'get',\n",
       " 'been',\n",
       " 'people',\n",
       " 'also',\n",
       " 'into',\n",
       " 'do',\n",
       " 'great',\n",
       " 'other',\n",
       " 'will',\n",
       " 'first',\n",
       " 'because',\n",
       " 'him',\n",
       " 'how',\n",
       " 'most',\n",
       " 'don',\n",
       " 'them',\n",
       " 'made',\n",
       " 'its',\n",
       " 'make',\n",
       " 'then',\n",
       " 'way',\n",
       " 'could',\n",
       " 'too',\n",
       " 'movies',\n",
       " 'after',\n",
       " 'any',\n",
       " 'characters',\n",
       " 'character',\n",
       " 'think',\n",
       " 'films',\n",
       " 'two',\n",
       " 'watch',\n",
       " 'being',\n",
       " 'many',\n",
       " 'plot',\n",
       " 'seen',\n",
       " 'never',\n",
       " 'where',\n",
       " 'love',\n",
       " 'life',\n",
       " 'little',\n",
       " 'acting',\n",
       " 'best',\n",
       " 'did',\n",
       " 'over',\n",
       " 'off',\n",
       " 'know',\n",
       " 'show',\n",
       " 'ever',\n",
       " 'does',\n",
       " 'man',\n",
       " 'better',\n",
       " 'your',\n",
       " 'here',\n",
       " 'end',\n",
       " 'scene',\n",
       " 'these',\n",
       " 'still',\n",
       " 'while',\n",
       " 'why',\n",
       " 'scenes',\n",
       " 'say',\n",
       " 'something',\n",
       " 'go',\n",
       " 've',\n",
       " 'm',\n",
       " 'should',\n",
       " 'such',\n",
       " 'back',\n",
       " 'through',\n",
       " 'real',\n",
       " 'those',\n",
       " 'now',\n",
       " 're',\n",
       " 'watching',\n",
       " 'doesn',\n",
       " 'thing',\n",
       " 'though',\n",
       " 'director',\n",
       " 'years',\n",
       " 'actors',\n",
       " 'funny',\n",
       " 'old',\n",
       " 'didn',\n",
       " 'another',\n",
       " 'new',\n",
       " 'nothing',\n",
       " 'makes',\n",
       " 'work',\n",
       " 'actually',\n",
       " 'going',\n",
       " 'before',\n",
       " '10',\n",
       " 'look',\n",
       " 'find',\n",
       " 'same',\n",
       " 'lot',\n",
       " 'few',\n",
       " 'part',\n",
       " 'every',\n",
       " 'again',\n",
       " 'world',\n",
       " 'cast',\n",
       " 'us',\n",
       " 'things',\n",
       " 'horror',\n",
       " 'quite',\n",
       " 'want',\n",
       " 'action',\n",
       " 'down',\n",
       " 'pretty',\n",
       " 'young',\n",
       " 'around',\n",
       " 'seems',\n",
       " 'fact',\n",
       " 'take',\n",
       " 'however',\n",
       " 'enough',\n",
       " 'got',\n",
       " 'long',\n",
       " 'both',\n",
       " 'thought',\n",
       " 'big',\n",
       " 'own',\n",
       " 'give',\n",
       " 'between',\n",
       " 'comedy',\n",
       " 'series',\n",
       " 'must',\n",
       " 'may',\n",
       " 'right',\n",
       " 'd',\n",
       " 'original',\n",
       " 'without',\n",
       " 'role',\n",
       " 'interesting',\n",
       " 'come',\n",
       " 'times',\n",
       " 'always',\n",
       " 'isn',\n",
       " 'guy',\n",
       " 'saw',\n",
       " 'whole',\n",
       " 'gets',\n",
       " 'least',\n",
       " 'point',\n",
       " 'almost',\n",
       " 'bit',\n",
       " '2',\n",
       " 'script',\n",
       " 'music',\n",
       " 'done',\n",
       " 'last',\n",
       " 'minutes',\n",
       " 'far',\n",
       " 'family',\n",
       " 'feel',\n",
       " 'since',\n",
       " 'making',\n",
       " 'll',\n",
       " 'girl',\n",
       " 'might',\n",
       " 'performance',\n",
       " 'anything',\n",
       " 'yet',\n",
       " 'away',\n",
       " 'probably',\n",
       " 'am',\n",
       " 'woman',\n",
       " 'kind',\n",
       " 'tv',\n",
       " 'hard',\n",
       " 'fun',\n",
       " 'rather',\n",
       " 'day',\n",
       " 'worst',\n",
       " 'sure',\n",
       " 'played',\n",
       " 'anyone',\n",
       " 'each',\n",
       " 'found',\n",
       " 'especially',\n",
       " 'having',\n",
       " 'our',\n",
       " 'trying',\n",
       " 'screen',\n",
       " 'looking',\n",
       " 'believe',\n",
       " 'different',\n",
       " 'although',\n",
       " 'place',\n",
       " 'course',\n",
       " 'goes',\n",
       " 'sense',\n",
       " 'set',\n",
       " 'comes',\n",
       " 'ending',\n",
       " 'maybe',\n",
       " 'shows',\n",
       " 'worth',\n",
       " 'american',\n",
       " 'three',\n",
       " 'money',\n",
       " 'dvd',\n",
       " 'put',\n",
       " 'looks',\n",
       " 'once',\n",
       " 'everything',\n",
       " 'actor',\n",
       " 'someone',\n",
       " 'let',\n",
       " 'plays',\n",
       " 'effects',\n",
       " 'main',\n",
       " 'john',\n",
       " 'wasn',\n",
       " 'year',\n",
       " 'together',\n",
       " 'reason',\n",
       " 'book',\n",
       " 'true',\n",
       " 'everyone',\n",
       " 'during',\n",
       " 'instead',\n",
       " 'takes',\n",
       " 'said',\n",
       " 'job',\n",
       " 'high',\n",
       " 'play',\n",
       " 'special',\n",
       " 'war',\n",
       " 'seem',\n",
       " 'night',\n",
       " 'watched',\n",
       " 'later',\n",
       " 'audience',\n",
       " '1',\n",
       " 'wife',\n",
       " 'himself',\n",
       " 'star',\n",
       " 'black',\n",
       " 'half',\n",
       " 'seeing',\n",
       " '3',\n",
       " 'left',\n",
       " 'death',\n",
       " 'idea',\n",
       " 'excellent',\n",
       " 'beautiful',\n",
       " 'shot',\n",
       " 'house',\n",
       " 'second',\n",
       " 'simply',\n",
       " 'father',\n",
       " 'used',\n",
       " 'men',\n",
       " 'dead',\n",
       " 'else',\n",
       " 'mind',\n",
       " 'version',\n",
       " 'less',\n",
       " 'completely',\n",
       " 'hollywood',\n",
       " 'nice',\n",
       " 'poor',\n",
       " 'fan',\n",
       " 'budget',\n",
       " 'women',\n",
       " 'help',\n",
       " 'home',\n",
       " 'line',\n",
       " 'sex',\n",
       " 'boring',\n",
       " 'performances',\n",
       " 'along',\n",
       " 'try',\n",
       " 'either',\n",
       " 'top',\n",
       " 'short',\n",
       " 'read',\n",
       " 'low',\n",
       " 'wrong',\n",
       " 'use',\n",
       " 'until',\n",
       " 'camera',\n",
       " 'friends',\n",
       " 'given',\n",
       " 'kids',\n",
       " 'couple',\n",
       " 'next',\n",
       " 'need',\n",
       " 'start',\n",
       " 'enjoy',\n",
       " 'full',\n",
       " 'classic',\n",
       " 'production',\n",
       " 'rest',\n",
       " 'truly',\n",
       " 'perhaps',\n",
       " 'stupid',\n",
       " 'awful',\n",
       " 'school',\n",
       " 'moments',\n",
       " 'video',\n",
       " 'mother',\n",
       " 'tell',\n",
       " 'mean',\n",
       " 'getting',\n",
       " 'face',\n",
       " 'keep',\n",
       " 'came',\n",
       " 'won',\n",
       " 'understand',\n",
       " 'small',\n",
       " 'terrible',\n",
       " 'others',\n",
       " 'recommend',\n",
       " 'name',\n",
       " 'style',\n",
       " 'playing',\n",
       " 'itself',\n",
       " 'boy',\n",
       " 'wonderful',\n",
       " 'doing',\n",
       " 'stars',\n",
       " 'remember',\n",
       " 'person',\n",
       " 'definitely',\n",
       " 'gives',\n",
       " 'often',\n",
       " 'lost',\n",
       " 'dialogue',\n",
       " 'written',\n",
       " 'live',\n",
       " 'early',\n",
       " 'lines',\n",
       " 'perfect',\n",
       " 'human',\n",
       " 'case',\n",
       " 'entertaining',\n",
       " 'head',\n",
       " 'yes',\n",
       " 'title',\n",
       " 'become',\n",
       " 'went',\n",
       " 'couldn',\n",
       " 'hope',\n",
       " 'episode',\n",
       " 'children',\n",
       " 'liked',\n",
       " 'friend',\n",
       " 'certainly',\n",
       " 'based',\n",
       " 'supposed',\n",
       " 'picture',\n",
       " 'piece',\n",
       " 'problem',\n",
       " 'finally',\n",
       " 'against',\n",
       " 'absolutely',\n",
       " 'oh',\n",
       " 'drama',\n",
       " 'sort',\n",
       " 'fans',\n",
       " 'several',\n",
       " 'cinema',\n",
       " 'overall',\n",
       " 'entire',\n",
       " 'felt',\n",
       " 'under',\n",
       " 'son',\n",
       " 'worse',\n",
       " 'laugh',\n",
       " 'called',\n",
       " 'evil',\n",
       " 'direction',\n",
       " 'lives',\n",
       " 'waste',\n",
       " 'killer',\n",
       " 'lead',\n",
       " 'humor',\n",
       " 'guys',\n",
       " '4',\n",
       " 'care',\n",
       " 'beginning',\n",
       " 'white',\n",
       " 'dark',\n",
       " 'game',\n",
       " 'despite',\n",
       " 'seemed',\n",
       " '5',\n",
       " 'final',\n",
       " 'becomes',\n",
       " 'wanted',\n",
       " 'unfortunately',\n",
       " 'mr',\n",
       " 'throughout',\n",
       " 'loved',\n",
       " 'totally',\n",
       " 'history',\n",
       " 'already',\n",
       " 'b',\n",
       " 'genre',\n",
       " 'turn',\n",
       " 'town',\n",
       " 'guess',\n",
       " 'fine',\n",
       " 'able',\n",
       " 'days',\n",
       " 'heart',\n",
       " 'city',\n",
       " 'flick',\n",
       " 'act',\n",
       " 'run',\n",
       " 'side',\n",
       " 'wants',\n",
       " 'today',\n",
       " 'quality',\n",
       " 'tries',\n",
       " 'child',\n",
       " 'hand',\n",
       " 'close',\n",
       " 'kill',\n",
       " 'horrible',\n",
       " 'past',\n",
       " 'sound',\n",
       " 'example',\n",
       " 'starts',\n",
       " 'writing',\n",
       " 'viewer',\n",
       " 'turns',\n",
       " 'themselves',\n",
       " 'amazing',\n",
       " 'enjoyed',\n",
       " 'etc',\n",
       " 'car',\n",
       " 'parts',\n",
       " 'behind',\n",
       " 'directed',\n",
       " 'works',\n",
       " 'expect',\n",
       " 'michael',\n",
       " 'killed',\n",
       " 'matter',\n",
       " 'daughter',\n",
       " 'favorite',\n",
       " 'soon',\n",
       " 'fight',\n",
       " 'kid',\n",
       " 'self',\n",
       " 'decent',\n",
       " 'stuff',\n",
       " 'gave',\n",
       " 'blood',\n",
       " 'sometimes',\n",
       " 'type',\n",
       " 'actress',\n",
       " 'eyes',\n",
       " 'thinking',\n",
       " 'group',\n",
       " 'girls',\n",
       " 'art',\n",
       " 'violence',\n",
       " 'obviously',\n",
       " 'brilliant',\n",
       " 'stop',\n",
       " 'stories',\n",
       " 'late',\n",
       " 'hour',\n",
       " 'known',\n",
       " 'myself',\n",
       " 'except',\n",
       " 'writer',\n",
       " 'happened',\n",
       " 'hero',\n",
       " 'says',\n",
       " 'god',\n",
       " 'feeling',\n",
       " 'highly',\n",
       " 'heard',\n",
       " 'coming',\n",
       " 'roles',\n",
       " 'extremely',\n",
       " 'police',\n",
       " 'took',\n",
       " 'happens',\n",
       " 'slow',\n",
       " 'leave',\n",
       " 'experience',\n",
       " 'moment',\n",
       " 'husband',\n",
       " 'anyway',\n",
       " 'voice',\n",
       " 'hell',\n",
       " 'wouldn',\n",
       " 'murder',\n",
       " 'attempt',\n",
       " 'involved',\n",
       " 'age',\n",
       " 'obvious',\n",
       " 'living',\n",
       " 'interest',\n",
       " 'including',\n",
       " 'score',\n",
       " 'strong',\n",
       " 'looked',\n",
       " 'taken',\n",
       " 'told',\n",
       " 'david',\n",
       " 'save',\n",
       " 'brother',\n",
       " 'ok',\n",
       " 'wonder',\n",
       " 'none',\n",
       " 'happen',\n",
       " 'cut',\n",
       " 'career',\n",
       " 'please',\n",
       " 'hours',\n",
       " 'cool',\n",
       " 'robert',\n",
       " 'chance',\n",
       " 'particularly',\n",
       " 'gore',\n",
       " 'james',\n",
       " 'cannot',\n",
       " 'simple',\n",
       " 'hit',\n",
       " 'across',\n",
       " 'ago',\n",
       " 'complete',\n",
       " 'exactly',\n",
       " 'lack',\n",
       " 'hilarious',\n",
       " 'crap',\n",
       " 'possible',\n",
       " 'annoying',\n",
       " 'alone',\n",
       " 'o',\n",
       " 'power',\n",
       " 'relationship',\n",
       " 'light',\n",
       " 'serious',\n",
       " 'sad',\n",
       " 'level',\n",
       " 'important',\n",
       " 'running',\n",
       " 'documentary',\n",
       " 'seriously',\n",
       " 'usually',\n",
       " 'whose',\n",
       " 'female',\n",
       " 'reality',\n",
       " 'ends',\n",
       " 'scary',\n",
       " 'somewhat',\n",
       " 'order',\n",
       " 'talent',\n",
       " 'happy',\n",
       " 'finds',\n",
       " 'taking',\n",
       " 'song',\n",
       " 'middle',\n",
       " 'number',\n",
       " 'shown',\n",
       " 'ridiculous',\n",
       " 'room',\n",
       " 'strange',\n",
       " 'change',\n",
       " 'call',\n",
       " 'basically',\n",
       " 'released',\n",
       " 'usual',\n",
       " 'body',\n",
       " 'opening',\n",
       " 'jokes',\n",
       " 'turned',\n",
       " 'mostly',\n",
       " 'english',\n",
       " 'country',\n",
       " 'wish',\n",
       " 'yourself',\n",
       " 'apparently',\n",
       " 'cinematography',\n",
       " 'opinion',\n",
       " 'silly',\n",
       " 'novel',\n",
       " 'attention',\n",
       " 'view',\n",
       " 'four',\n",
       " 'started',\n",
       " 'word',\n",
       " 'saying',\n",
       " 'jack',\n",
       " 'disappointed',\n",
       " 'miss',\n",
       " 'sequel',\n",
       " 'single',\n",
       " 'talking',\n",
       " 'huge',\n",
       " 'thriller',\n",
       " 'future',\n",
       " 'clich',\n",
       " 'shots',\n",
       " 'words',\n",
       " 'major',\n",
       " 'cheap',\n",
       " 'straight',\n",
       " 'non',\n",
       " 'clearly',\n",
       " 'rating',\n",
       " 'modern',\n",
       " 'beyond',\n",
       " 'knows',\n",
       " 'knew',\n",
       " 'ones',\n",
       " 'due',\n",
       " 'problems',\n",
       " 'fast',\n",
       " 'events',\n",
       " 'british',\n",
       " 'sets',\n",
       " 'king',\n",
       " 'talk',\n",
       " '8',\n",
       " 'tells',\n",
       " 'comic',\n",
       " '7',\n",
       " 'french',\n",
       " 'parents',\n",
       " 'bring',\n",
       " 'die',\n",
       " 'easily',\n",
       " 'aren',\n",
       " 'entertainment',\n",
       " 'local',\n",
       " 'earth',\n",
       " 'add',\n",
       " 'class',\n",
       " 'sequence',\n",
       " 'upon',\n",
       " 'george',\n",
       " 'above',\n",
       " 'musical',\n",
       " 'television',\n",
       " 'within',\n",
       " 'giving',\n",
       " 'falls',\n",
       " 'similar',\n",
       " 'york',\n",
       " 'storyline',\n",
       " 'ten',\n",
       " 'supporting',\n",
       " 'clear',\n",
       " 'mystery',\n",
       " 'haven',\n",
       " 'easy',\n",
       " 'appears',\n",
       " 'hate',\n",
       " 'romantic',\n",
       " 'five',\n",
       " 'predictable',\n",
       " 'review',\n",
       " 'near',\n",
       " 'typical',\n",
       " 'lots',\n",
       " 'ways',\n",
       " 'bunch',\n",
       " 'team',\n",
       " 'enjoyable',\n",
       " 'begins',\n",
       " 'named',\n",
       " 'dialog',\n",
       " 'general',\n",
       " 'stand',\n",
       " 'crime',\n",
       " 'working',\n",
       " 'elements',\n",
       " 'mention',\n",
       " 'eye',\n",
       " 'message',\n",
       " 'theme',\n",
       " 'filmed',\n",
       " 'richard',\n",
       " 'episodes',\n",
       " 'certain',\n",
       " 'avoid',\n",
       " 'points',\n",
       " 'e',\n",
       " 'songs',\n",
       " 'red',\n",
       " 'america',\n",
       " 'tale',\n",
       " 'sorry',\n",
       " 'whether',\n",
       " 'gay',\n",
       " 'release',\n",
       " 'dull',\n",
       " 'surprised',\n",
       " 'moving',\n",
       " 'among',\n",
       " 'tom',\n",
       " 'viewers',\n",
       " 'stay',\n",
       " 'de',\n",
       " 'using',\n",
       " 'needs',\n",
       " 'fall',\n",
       " 'minute',\n",
       " 'effort',\n",
       " 'feels',\n",
       " 'gone',\n",
       " 'space',\n",
       " 'lee',\n",
       " 'leads',\n",
       " 'kept',\n",
       " 'paul',\n",
       " '9',\n",
       " 'nearly',\n",
       " 'theater',\n",
       " 'tried',\n",
       " 'herself',\n",
       " 'comments',\n",
       " 'means',\n",
       " 'peter',\n",
       " 'period',\n",
       " 'showing',\n",
       " 'third',\n",
       " 'truth',\n",
       " 'sister',\n",
       " 'brought',\n",
       " 'suspense',\n",
       " 'buy',\n",
       " 'doubt',\n",
       " 'soundtrack',\n",
       " 'somehow',\n",
       " 'killing',\n",
       " 'lady',\n",
       " 'feature',\n",
       " 'follow',\n",
       " 'sequences',\n",
       " 'viewing',\n",
       " 'fantastic',\n",
       " 'editing',\n",
       " 'form',\n",
       " 'famous',\n",
       " 'material',\n",
       " 'realistic',\n",
       " 'rent',\n",
       " 'average',\n",
       " 'cop',\n",
       " 'okay',\n",
       " 'dog',\n",
       " 'check',\n",
       " 'monster',\n",
       " 'whatever',\n",
       " 'rock',\n",
       " 'reviews',\n",
       " 'imagine',\n",
       " 'move',\n",
       " 'figure',\n",
       " 'oscar',\n",
       " 'surprise',\n",
       " 'forget',\n",
       " 'premise',\n",
       " 'lame',\n",
       " 'believable',\n",
       " 'weak',\n",
       " 'fi',\n",
       " 'indeed',\n",
       " 'animation',\n",
       " 'deal',\n",
       " 'poorly',\n",
       " 'sci',\n",
       " '20',\n",
       " 'free',\n",
       " 'possibly',\n",
       " 'actual',\n",
       " 'expected',\n",
       " 'learn',\n",
       " 'hear',\n",
       " 'eventually',\n",
       " 'dr',\n",
       " 'stage',\n",
       " 'forced',\n",
       " 'sexual',\n",
       " 'note',\n",
       " 'atmosphere',\n",
       " 'deep',\n",
       " 'society',\n",
       " 'greatest',\n",
       " 'sit',\n",
       " 'otherwise',\n",
       " 'open',\n",
       " 'wait',\n",
       " 'leaves',\n",
       " 'difficult',\n",
       " 'question',\n",
       " 'romance',\n",
       " 'decided',\n",
       " 'screenplay',\n",
       " 'begin',\n",
       " 'reading',\n",
       " 'plus',\n",
       " 'joe',\n",
       " 'situation',\n",
       " 'western',\n",
       " 'became',\n",
       " 'subject',\n",
       " 'particular',\n",
       " 'earlier',\n",
       " 'hot',\n",
       " 'nor',\n",
       " 'male',\n",
       " 'towards',\n",
       " 'box',\n",
       " 'gun',\n",
       " 'crew',\n",
       " 'brothers',\n",
       " 'interested',\n",
       " 'personal',\n",
       " 'acted',\n",
       " 'street',\n",
       " 'meet',\n",
       " 'credits',\n",
       " 'previous',\n",
       " 'imdb',\n",
       " 'cheesy',\n",
       " 'footage',\n",
       " 'business',\n",
       " 'powerful',\n",
       " 'memorable',\n",
       " 'worked',\n",
       " 'battle',\n",
       " 'shame',\n",
       " 'writers',\n",
       " 'mess',\n",
       " 'effect',\n",
       " 'laughs',\n",
       " 'whom',\n",
       " 'features',\n",
       " 'result',\n",
       " 'dramatic',\n",
       " 'season',\n",
       " 'older',\n",
       " 'air',\n",
       " 'setting',\n",
       " 'perfectly',\n",
       " 'unless',\n",
       " 'era',\n",
       " 'quickly',\n",
       " 'needed',\n",
       " 'keeps',\n",
       " 'nature',\n",
       " 'hands',\n",
       " 'boys',\n",
       " 'baby',\n",
       " 'bill',\n",
       " 'crazy',\n",
       " 'badly',\n",
       " 'total',\n",
       " 'background',\n",
       " 'directing',\n",
       " 'realize',\n",
       " 'emotional',\n",
       " 'mark',\n",
       " 'comment',\n",
       " 'forward',\n",
       " 'present',\n",
       " 'japanese',\n",
       " 'appear',\n",
       " 'twist',\n",
       " 'development',\n",
       " 'girlfriend',\n",
       " 'pay',\n",
       " 'telling',\n",
       " 'write',\n",
       " 'rich',\n",
       " 'superb',\n",
       " 'various',\n",
       " 'meets',\n",
       " 'unique',\n",
       " 'dance',\n",
       " 'weird',\n",
       " 'island',\n",
       " 'william',\n",
       " 'directors',\n",
       " 'plenty',\n",
       " 'secret',\n",
       " 'break',\n",
       " 'fighting',\n",
       " 'disney',\n",
       " '30',\n",
       " 'front',\n",
       " 'apart',\n",
       " 'brings',\n",
       " 'sounds',\n",
       " 'masterpiece',\n",
       " 'doctor',\n",
       " 'fairly',\n",
       " 'incredibly',\n",
       " 'villain',\n",
       " 'outside',\n",
       " 'dream',\n",
       " 'c',\n",
       " 'married',\n",
       " 'missing',\n",
       " 'leading',\n",
       " 'party',\n",
       " 'manages',\n",
       " 'return',\n",
       " 'beauty',\n",
       " 'remake',\n",
       " 'reasons',\n",
       " 'inside',\n",
       " 'zombie',\n",
       " 'fantasy',\n",
       " 'admit',\n",
       " 'list',\n",
       " 'rate',\n",
       " 'ideas',\n",
       " 'political',\n",
       " 'create',\n",
       " 'ask',\n",
       " 'creepy',\n",
       " 'meant',\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2word\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    #\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    #\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    #\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    #\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    #\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "        if counter%1000. == 0. :\n",
    "            print('Review {0} of {1}'.format(counter, len(reviews)))\n",
    "       #\n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCleanReviews(reviews):\n",
    "    clean_reviews = []\n",
    "    for review in reviews[\"review\"]:\n",
    "        clean_reviews.append(review_to_wordlist( review, remove_stopwords=True ))\n",
    "    return clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for training reviews\n",
      "Review 0.0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 3000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 5000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 7000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 9000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 11000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 13000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 15000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 17000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 19000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 21000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 23000.0 of 25000\n",
      "Review 24000.0 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0.0 of 25000\n",
      "Review 1000.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 3000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 5000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 7000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 9000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 11000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 13000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 15000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 17000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 19000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 21000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 23000.0 of 25000\n",
      "Review 24000.0 of 25000\n",
      "Fitting a random forest to labeled training data...\n",
      "Wrote Word2Vec_AverageVectors.csv\n"
     ]
    }
   ],
   "source": [
    "# ****** Create average vectors for the training and test sets\n",
    "#\n",
    "print(\"Creating average feature vecs for training reviews\")\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( getCleanReviews(test), model, num_features )\n",
    "\n",
    "\n",
    "    # ****** Fit a random forest to the training set, then make predictions\n",
    "    #\n",
    "    # Fit a random forest to the training data, using 100 trees\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "    # Test & extract results\n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "    # Write the test results\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )\n",
    "print(\"Wrote Word2Vec_AverageVectors.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
